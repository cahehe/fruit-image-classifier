{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet_fruits_training.py\n",
    "# Fine-tune ResNet-50 on Fruits-360 (or similar) using labels CSV metadata.\n",
    "# - Expects a labels CSV with columns: split (train/test) or separate train/test CSVs\n",
    "# - Produces checkpoints and prints training/validation metrics\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import logging\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "import types\n",
    "import __main__\n",
    "import torch\n",
    "import sys\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from checkpoint_utils import save_checkpoint, find_latest_checkpoint, load_checkpoint\n",
    "\n",
    "# ---------------------- Dataset ----------------------\n",
    "class FruitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DataFrame expected columns:\n",
    "      - 'relative_path' or 'filename' (path relative to ROOT_IMAGE_DIR)\n",
    "      - 'label_id' or 'label_index' (integer class)\n",
    "      - optionally 'split'\n",
    "    \"\"\"\n",
    "    def __init__(self, df, root_dir, transform=None, path_col=\"relative_path\", label_col=\"label_id\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        rel_path = row[self.path_col]\n",
    "        image_path = self.root_dir.joinpath(rel_path)\n",
    "        # Robust open\n",
    "        with Image.open(image_path) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                im = self.transform(im)\n",
    "        label = int(row[self.label_col])\n",
    "        return im, label\n",
    "\n",
    "# ---------------------- Training / Eval functions ----------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, logger):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)        \n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    logger.info(f\"Training Epoch finished - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device, logger):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)            \n",
    "\n",
    "    logger.info(f\"Eval Epoch finished - Running_loss: {running_loss:.4f}, Running_corrects: {running_corrects:.4f}, Total: {total:.4f}\")\n",
    "    return running_loss / total, running_corrects / total\n",
    "\n",
    "def main():\n",
    "    # ---------------------- Logging ----------------------\n",
    "    log_level = os.getenv(\"PYTHONLOGLEVEL\", \"INFO\").upper()\n",
    "    logging.basicConfig(\n",
    "      level=getattr(logging, log_level),      \n",
    "      format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "      handlers=[logging.StreamHandler(sys.stdout)],\n",
    "      force=True  # Only works in Python 3.8+\n",
    "    )\n",
    "    logger = logging.getLogger(\"train\")\n",
    "\n",
    "    # ---------------------- Configuration ----------------------\n",
    "    logger.info(\"Configuring\")\n",
    "    '''ROOT_IMAGE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/Fruits-360/fruits-360_100x100/fruits-360\"\n",
    "    LABELS_CSV = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier/labels.csv\"\n",
    "    CLASSES_CSV = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier/classes.csv\"\n",
    "    OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier\"'''\n",
    "    ROOT_IMAGE_DIR = \"/Users/carloshehe/Desktop/Fruits-360/fruits-360_100x100/fruits-360\"\n",
    "    LABELS_CSV = \"/Users/carloshehe/Desktop/Projects/fruit-image-classifier/labels.csv\"\n",
    "    CLASSES_CSV = \"/Users/carloshehe/Desktop/Projects/fruit-image-classifier/classes.csv\"\n",
    "    OUTPUT_DIR = \"/Users/carloshehe/Desktop/Projects/fruit-image-classifier\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_EPOCHS = 8\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    IMAGE_SIZE = 224  # ResNet default\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # ImageNet mean/std (pretrained ResNet expects these)\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD  = [0.229, 0.224, 0.225]    \n",
    "\n",
    "    # ---------------------- Transforms ----------------------\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "    # ---------------------- Load CSV and split ----------------------\n",
    "    df = pd.read_csv(LABELS_CSV)\n",
    "    logger.info(f\"Loaded labels CSV with columns: {df.columns.tolist()} (rows={len(df)})\")\n",
    "\n",
    "    # candidate column names\n",
    "    path_col_candidates = ['relative_path', 'filename', 'file_name', 'path']\n",
    "    label_col_candidates = ['label_id', 'label_index', 'label', 'class_index']\n",
    "\n",
    "    path_col = next((c for c in path_col_candidates if c in df.columns), None)\n",
    "    label_col = next((c for c in label_col_candidates if c in df.columns), None)\n",
    "\n",
    "    if path_col is None or label_col is None:\n",
    "        raise ValueError(f\"Could not find path or label column. CSV columns: {df.columns.tolist()}\")\n",
    "\n",
    "    logger.info(f\"Using path_col='{path_col}', label_col='{label_col}'\")\n",
    "    logger.debug(f\"DataFrame columns before sampling: {df.columns.tolist()}\")\n",
    "    # ---------------------- Optional quick-sampling for tests ----------------------\n",
    "    # SAMPLE_FRACTION can be set via env var e.g. SAMPLE_FRACTION=0.01 python ...\n",
    "    try:\n",
    "        SAMPLE_FRACTION = float(os.environ.get(\"SAMPLE_FRACTION\", \"0.01\"))\n",
    "    except Exception:\n",
    "        SAMPLE_FRACTION = 0.01\n",
    "\n",
    "    if SAMPLE_FRACTION <= 0 or SAMPLE_FRACTION > 1:\n",
    "        logger.warning(f\"Invalid SAMPLE_FRACTION={SAMPLE_FRACTION}; skipping sampling and using full dataset.\")\n",
    "        SAMPLE_FRACTION = 1.0\n",
    "\n",
    "    if SAMPLE_FRACTION < 1.0:\n",
    "        logger.info(f\"Sampling a fraction of the dataset for quick tests: SAMPLE_FRACTION={SAMPLE_FRACTION}\")\n",
    "        # Perform per-class stratified sampling and ensure at least 1 sample per class\n",
    "        def _sample_group(g):\n",
    "            n = max(1, int(len(g) * SAMPLE_FRACTION))\n",
    "            # If group has fewer rows than n, sample with replacement is not desired; just return the group\n",
    "            if n >= len(g):\n",
    "                return g\n",
    "            return g.sample(n=n, random_state=42)\n",
    "        \n",
    "        '''for name, g in df.groupby(label_col):\n",
    "            print(\"GROUP NAME:\", name)\n",
    "            print(\"GROUP COLUMNS:\", g.columns.tolist())   # you'll see label_index is present here\n",
    "            print(\"GROUP PASSED TO _sample_group (when include_groups=False) will NOT contain label_index\")\n",
    "            break\n",
    "        sampled_df = df.groupby(label_col, group_keys=False).apply(_sample_group, include_groups=False).reset_index(drop=False)'''\n",
    "        sampled_parts = []\n",
    "        for _, group in df.groupby(label_col):\n",
    "            n = max(1, int(len(group) * SAMPLE_FRACTION))\n",
    "            sampled_parts.append(group if n >= len(group) else group.sample(n=n, random_state=42))\n",
    "\n",
    "        sampled_df = pd.concat(sampled_parts, ignore_index=True)\n",
    "        df = sampled_df\n",
    "        logger.info(f\"Sampled dataset rows: {len(sampled_df)} (original {len(df)})\")\n",
    "        # replace df with sampled_df for downstream splitting\n",
    "        df = sampled_df\n",
    "\n",
    "    logger.debug(f\"DataFrame columns after sampling: {df.columns.tolist()}\")\n",
    "    # If 'split' exists, normalize and use it; otherwise do stratified split\n",
    "    if 'split' in df.columns:\n",
    "        df['split_norm'] = df['split'].astype(str).str.strip().str.lower()\n",
    "        def map_split(s):\n",
    "            if s.startswith('train'):\n",
    "                return 'train'\n",
    "            if s.startswith('test'):\n",
    "                return 'test'\n",
    "            if s in ['val', 'valid', 'validation']:\n",
    "                return 'val'\n",
    "            return s\n",
    "        df['split_norm'] = df['split_norm'].apply(map_split)\n",
    "        logger.info(f\"Split value counts (normalized):\\\\n{df['split_norm'].value_counts()}\")\n",
    "        if 'train' in df['split_norm'].values or 'test' in df['split_norm'].values or 'val' in df['split_norm'].values:\n",
    "            train_df = df[df['split_norm'] == 'train'].reset_index(drop=True)\n",
    "            val_df = df[df['split_norm'].isin(['test', 'val'])].reset_index(drop=True)\n",
    "            # optional: if there is also explicit 'test' split and it's separate, you can load it here\n",
    "            test_df = df[df['split_norm'] == 'test'].reset_index(drop=True)\n",
    "        else:\n",
    "            logger.warning(\"split column present but not standard; falling back to stratified split.\")\n",
    "            train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n",
    "            train_df = train_df.reset_index(drop=True)\n",
    "            val_df = val_df.reset_index(drop=True)\n",
    "            test_df = pd.DataFrame([], columns=df.columns)\n",
    "    else:\n",
    "        logger.info(\"No 'split' column found â€” performing a stratified train/val split (80/20).\")\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        val_df = val_df.reset_index(drop=True)\n",
    "        test_df = pd.DataFrame([], columns=df.columns)\n",
    "\n",
    "    # If test_df empty, you can set it equal to val_df or leave it empty - here we keep it empty if not provided.\n",
    "    logger.info(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "    logger.debug(f\"DataFrame columns after sampling and split: {df.columns.tolist()}\")\n",
    "\n",
    "    # ---------------------- Build datasets / loaders ----------------------\n",
    "    train_ds = FruitDataset(train_df, ROOT_IMAGE_DIR, transform=train_transform, path_col=path_col, label_col=label_col)\n",
    "    val_ds   = FruitDataset(val_df,   ROOT_IMAGE_DIR, transform=val_transform,   path_col=path_col, label_col=label_col)\n",
    "    test_ds  = FruitDataset(test_df,  ROOT_IMAGE_DIR, transform=val_transform,   path_col=path_col, label_col=label_col) if len(test_df)>0 else None\n",
    "\n",
    "    # guard\n",
    "    if len(train_ds) == 0:\n",
    "        raise ValueError(\"Training dataset is empty. Check LABELS_CSV and 'split' values.\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=(DEVICE.type=='cuda'))\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type=='cuda'))\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type=='cuda')) if test_ds is not None else None\n",
    "\n",
    "    # ---------------------- Model setup ----------------------\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    # num classes\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not found in DataFrame columns: {df.columns.tolist()}\")\n",
    "    NUM_CLASSES = int(df[label_col].nunique())\n",
    "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # ---------------------- Optionally freeze backbone ----------------------\n",
    "    def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "        if feature_extracting:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    FEATURE_EXTRACT = True\n",
    "    set_parameter_requires_grad(model, feature_extracting=FEATURE_EXTRACT)\n",
    "\n",
    "    # ---------------------- Loss, Optimizer, Scheduler ----------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(params_to_update, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "# ---------------------- Resume / checkpoint setup ----------------------\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0.0\n",
    "    ckpt = find_latest_checkpoint(OUTPUT_DIR, logger)\n",
    "    if ckpt:\n",
    "        try:            \n",
    "            logger.info(f\"Found existing checkpoint: {ckpt}. Resuming training.\")\n",
    "            start_epoch, best_val_acc = load_checkpoint(ckpt, model, logger, optimizer=optimizer, scheduler=scheduler, device=DEVICE)            \n",
    "            logger.info(f\"Resuming from checkpoint. start_epoch={start_epoch}, best_val_acc={best_val_acc:.4f}\")            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load checkpoint {ckpt}: {e}\\\\nStarting from scratch.\")\n",
    "\n",
    "    # ---------------------- Training loop ----------------------\n",
    "    try:\n",
    "        for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "            logger.info(f\"Starting epoch {epoch} out of {NUM_EPOCHS}\")\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, logger)\n",
    "\n",
    "            # Optional quick eval on training set for sanity-check (small overhead)\n",
    "            train_eval_loss, train_eval_acc = eval_one_epoch(model, train_loader, criterion, DEVICE, logger)\n",
    "\n",
    "            val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, DEVICE, logger)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}/{NUM_EPOCHS}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  \"\n",
    "                        f\"eval_train_loss={train_eval_loss:.4f} eval_train_acc={train_eval_acc:.4f}  \"\n",
    "                        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "            ckpt_state = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "                \"best_val_acc\": best_val_acc\n",
    "            }\n",
    "            # save latest checkpoint always\n",
    "            save_checkpoint(ckpt_state, os.path.join(OUTPUT_DIR, \"latest.pth\"), logger)\n",
    "\n",
    "            # save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_path = os.path.join(OUTPUT_DIR, f\"best_epoch_{epoch+1:03d}_valacc_{val_acc:.4f}.pth\")\n",
    "                save_checkpoint(ckpt_state, best_path, logger)\n",
    "                logger.info(f\"Saved new best model to {best_path} (val_acc={val_acc:.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Save latest on exception to avoid losing progress\n",
    "        tb = traceback.format_exc()\n",
    "        logger.error(f\"Training failed unexpectedly: {e}\\\\n{tb}\")\n",
    "        try:\n",
    "            save_checkpoint({\n",
    "                \"epoch\": epoch + 1 if 'epoch' in locals() else 0,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "                \"best_val_acc\": best_val_acc\n",
    "            }, os.path.join(OUTPUT_DIR, \"latest_on_error.pth\"), logger)\n",
    "            logger.info(\"Saved latest checkpoint to latest_on_error.pth\")\n",
    "        except Exception as se:\n",
    "            logger.error(f\"Failed to save checkpoint on error: {se}\")\n",
    "        raise\n",
    "\n",
    "    # ---------------------- Final evaluation on test set (optional) ----------------------\n",
    "    # If a best model was saved, load it for test evaluation\n",
    "    best_ckpt = None\n",
    "    best_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"best_epoch_*.pth\")), key=os.path.getmtime, reverse=True)\n",
    "    if best_files:\n",
    "        best_ckpt = best_files[0]\n",
    "    elif os.path.exists(os.path.join(OUTPUT_DIR, \"latest.pth\")):\n",
    "        best_ckpt = os.path.join(OUTPUT_DIR, \"latest.pth\")\n",
    "\n",
    "    if best_ckpt:\n",
    "        logger.info(f\"Loading best checkpoint for final evaluation: {best_ckpt}\")\n",
    "        load_checkpoint(best_ckpt, model, logger, device=DEVICE)\n",
    "\n",
    "    if test_loader is not None:\n",
    "        test_loss, test_acc = eval_one_epoch(model, test_loader, criterion, DEVICE, logger)\n",
    "        logger.info(f\"TEST final: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
    "    else:\n",
    "        logger.info(\"No test split provided - skipping final test evaluation.\")\n",
    "\n",
    "    # ---------------------- Save class map for inference convenience ----------------------\n",
    "    if os.path.exists(CLASSES_CSV):\n",
    "        logger.info(f\"classes CSV exists at {CLASSES_CSV}\")\n",
    "    else:\n",
    "        # write small classes CSV mapping if not present\n",
    "        uniq = sorted(df[label_col].unique())\n",
    "        classes_df = pd.DataFrame({\"class_index\": list(range(len(uniq))), \"class_name\": [str(x) for x in uniq]})\n",
    "        classes_df.to_csv(CLASSES_CSV, index=False)\n",
    "        logger.info(f\"Wrote fallback classes CSV to {CLASSES_CSV}\")\n",
    "\n",
    "    logger.info(\"Training script finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
