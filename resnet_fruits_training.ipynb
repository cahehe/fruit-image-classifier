{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":23404,"status":"ok","timestamp":1760801926519,"user":{"displayName":"Carlos","userId":"17576890038933523144"},"user_tz":240},"id":"NvKsIOeiyNnM","outputId":"71f6e41a-7ce7-4044-f5ab-6e3e41b07047"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd /content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier\n","%pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"id":"ZIglq_PpxPsG","lines_to_next_cell":2,"outputId":"3c01b522-fef9-4b15-a77b-076ab1a96827","executionInfo":{"status":"error","timestamp":1760768972528,"user_tz":240,"elapsed":29709,"user":{"displayName":"Carlos","userId":"17576890038933523144"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Configuration step\n","Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 97.8M/97.8M [00:00<00:00, 163MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["loaded existing ckpt\n","start_epoch:2 best_val_acc:0.9691283778365809\n","Resuming from checkpoint. start_epoch=2, best_val_acc=0.9691\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1935413528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1935413528.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;31m# Optional quick eval on training set for sanity-check (small overhead)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1935413528.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# resnet_fruits_training.py\n","# Fine-tune ResNet-50 on Fruits-360 (or similar) using labels CSV metadata.\n","# - Expects a labels CSV with columns: split (train/test) or separate train/test CSVs\n","# - Produces checkpoints and prints training/validation metrics\n","import os\n","import glob\n","from pathlib import Path\n","import pandas as pd\n","from PIL import Image\n","import logging\n","import traceback\n","from collections import OrderedDict\n","import types\n","import __main__\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from sklearn.model_selection import train_test_split\n","\n","# ---------------------- Dataset ----------------------\n","class FruitDataset(Dataset):\n","    \"\"\"\n","    DataFrame expected columns:\n","      - 'relative_path' or 'filename' (path relative to ROOT_IMAGE_DIR)\n","      - 'label_id' or 'label_index' (integer class)\n","      - optionally 'split'\n","    \"\"\"\n","    def __init__(self, df, root_dir, transform=None, path_col=\"relative_path\", label_col=\"label_id\"):\n","        self.df = df.reset_index(drop=True)\n","        self.root_dir = Path(root_dir)\n","        self.transform = transform\n","        self.path_col = path_col\n","        self.label_col = label_col\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        rel_path = row[self.path_col]\n","        image_path = self.root_dir.joinpath(rel_path)\n","        # Robust open\n","        with Image.open(image_path) as im:\n","            im = im.convert(\"RGB\")\n","            if self.transform:\n","                im = self.transform(im)\n","        label = int(row[self.label_col])\n","        return im, label\n","\n","# ---------------------- Helpers: checkpoints ----------------------\n","def save_checkpoint(state, path):\n","    os.makedirs(os.path.dirname(path), exist_ok=True)\n","    torch.save(state, path)\n","\n","def find_latest_checkpoint(folder):\n","    latest = os.path.join(folder, \"latest.pth\")\n","    if os.path.exists(latest):\n","        return latest\n","    files = glob.glob(os.path.join(folder, \"*.pth\"))\n","    if not files:\n","        return None\n","    files = sorted(files, key=os.path.getmtime, reverse=True)\n","    return files[0]\n","\n","def load_checkpoint(path, model, optimizer=None, scheduler=None, device=torch.device(\"cpu\")):\n","    # ensure dummy logger to allow unpickling if needed\n","    if not hasattr(__main__, \"logger\"):\n","        __main__.logger = types.SimpleNamespace(\n","            info=lambda *a, **k: None,\n","            warning=lambda *a, **k: None,\n","            debug=lambda *a, **k: None\n","        )\n","\n","    try:\n","        ckpt = torch.load(path, map_location=device)\n","    except Exception as e:\n","        print(f\"Warning: torch.load failed ({e}), retrying with dummy logger...\")\n","        __main__.logger = types.SimpleNamespace(\n","            info=lambda *a, **k: None,\n","            warning=lambda *a, **k: None,\n","            debug=lambda *a, **k: None\n","        )\n","        ckpt = torch.load(path, map_location=device)\n","\n","    # Extract state dict from checkpoint\n","    state = ckpt.get(\"model_state_dict\", ckpt)\n","\n","    # Strip 'module.' prefix if saved with DataParallel\n","    new_state = OrderedDict()\n","    for k, v in state.items():\n","        name = k\n","        if k.startswith(\"module.\"):\n","            name = k[len(\"module.\"):]\n","        new_state[name] = v\n","\n","    # Load weights into the model\n","    model.load_state_dict(new_state, strict=False)\n","\n","    # Optionally load optimizer state\n","    if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n","        try:\n","            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n","        except Exception as e:\n","            print(\"Could not load optimizer state:\", e)\n","\n","    # Optionally load scheduler state\n","    if scheduler is not None and \"scheduler_state_dict\" in ckpt and ckpt[\"scheduler_state_dict\"] is not None:\n","        try:\n","            scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n","        except Exception as e:\n","            print(\"Could not load scheduler state:\", e)\n","\n","    start_epoch = ckpt.get(\"epoch\", 0)\n","    best_val_acc = ckpt.get(\"best_val_acc\", 0.0)\n","    return start_epoch, best_val_acc\n","\n","# ---------------------- Training / Eval functions ----------------------\n","def train_one_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    total = 0\n","    for imgs, labels in loader:\n","        imgs = imgs.to(device, non_blocking=True)\n","        labels = labels.to(device, non_blocking=True)\n","\n","        outputs = model(imgs)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * imgs.size(0)\n","        preds = outputs.argmax(dim=1)\n","        running_corrects += (preds == labels).sum().item()\n","        total += imgs.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = running_corrects / total\n","    print(f\"Training Epoch finished - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n","    return epoch_loss, epoch_acc\n","\n","def eval_one_epoch(model, loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    total = 0\n","    with torch.no_grad():\n","        for imgs, labels in loader:\n","            imgs = imgs.to(device, non_blocking=True)\n","            labels = labels.to(device, non_blocking=True)\n","\n","            outputs = model(imgs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * imgs.size(0)\n","            preds = outputs.argmax(dim=1)\n","            running_corrects += (preds == labels).sum().item()\n","            total += imgs.size(0)\n","            #print(f\"Evaluating Epoch. Values so far - Running Loss: {running_loss:.4f}, Running Corrects: {running_corrects:.4f}, Total: {total}\")\n","\n","    print(f\"Eval Epoch finished - Running_loss: {running_loss:.4f}, Running_corrects: {running_corrects:.4f}, Total: {total:.4f}\")\n","    return running_loss / total, running_corrects / total\n","\n","def main():\n","    # ---------------------- Configuration ----------------------\n","    print(\"Configuration step\")\n","    ROOT_IMAGE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/Fruits-360/fruits-360_100x100/fruits-360\"\n","    LABELS_CSV = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier/labels.csv\"\n","    CLASSES_CSV = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier/classes.csv\"\n","    OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Projects/Fruits Image Classifier/fruit-image-classifier\"\n","    '''ROOT_IMAGE_DIR = \"/Users/carloshehe/Desktop/Fruits-360/fruits-360_100x100/fruits-360\"\n","    LABELS_CSV = \"/Users/carloshehe/Desktop/Projects/fruit-image-classifier/labels.csv\"\n","    CLASSES_CSV = \"/Users/carloshehe/Desktop/Projects/fruit-image-classifier/classes.csv\"\n","    OUTPUT_DIR = \"/Users/carloshehe/Desktop/Projects/fruit-image-classifier\"'''\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","    BATCH_SIZE = 64\n","    NUM_WORKERS = 4\n","    NUM_EPOCHS = 8\n","    LEARNING_RATE = 1e-3\n","    WEIGHT_DECAY = 1e-4\n","    IMAGE_SIZE = 224  # ResNet default\n","\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Using device:\", DEVICE)\n","\n","    # ImageNet mean/std (pretrained ResNet expects these)\n","    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","    IMAGENET_STD  = [0.229, 0.224, 0.225]\n","\n","    # ---------------------- Logging ----------------------\n","    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n","    logger = logging.getLogger(\"train\")\n","\n","    # ---------------------- Transforms ----------------------\n","    train_transform = transforms.Compose([\n","        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n","        transforms.ToTensor(),\n","        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n","    ])\n","\n","    val_transform = transforms.Compose([\n","        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n","    ])\n","\n","    # ---------------------- Load CSV and split ----------------------\n","    df = pd.read_csv(LABELS_CSV)\n","    logger.info(f\"Loaded labels CSV with columns: {df.columns.tolist()} (rows={len(df)})\")\n","\n","    # candidate column names\n","    path_col_candidates = ['relative_path', 'filename', 'file_name', 'path']\n","    label_col_candidates = ['label_id', 'label_index', 'label', 'class_index']\n","\n","    path_col = next((c for c in path_col_candidates if c in df.columns), None)\n","    label_col = next((c for c in label_col_candidates if c in df.columns), None)\n","\n","    if path_col is None or label_col is None:\n","        raise ValueError(f\"Could not find path or label column. CSV columns: {df.columns.tolist()}\")\n","\n","    logger.info(f\"Using path_col='{path_col}', label_col='{label_col}'\")\n","\n","    # If 'split' exists, normalize and use it; otherwise do stratified split\n","    if 'split' in df.columns:\n","        df['split_norm'] = df['split'].astype(str).str.strip().str.lower()\n","        def map_split(s):\n","            if s.startswith('train'):\n","                return 'train'\n","            if s.startswith('test'):\n","                return 'test'\n","            if s in ['val', 'valid', 'validation']:\n","                return 'val'\n","            return s\n","        df['split_norm'] = df['split_norm'].apply(map_split)\n","        logger.info(f\"Split value counts (normalized):\\n{df['split_norm'].value_counts()}\")\n","        if 'train' in df['split_norm'].values or 'test' in df['split_norm'].values or 'val' in df['split_norm'].values:\n","            train_df = df[df['split_norm'] == 'train'].reset_index(drop=True)\n","            val_df = df[df['split_norm'].isin(['test', 'val'])].reset_index(drop=True)\n","            # optional: if there is also explicit 'test' split and it's separate, you can load it here\n","            test_df = df[df['split_norm'] == 'test'].reset_index(drop=True)\n","        else:\n","            logger.warning(\"split column present but not standard; falling back to stratified split.\")\n","            train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n","            train_df = train_df.reset_index(drop=True)\n","            val_df = val_df.reset_index(drop=True)\n","            test_df = pd.DataFrame([], columns=df.columns)\n","    else:\n","        logger.info(\"No 'split' column found — performing a stratified train/val split (80/20).\")\n","        train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n","        train_df = train_df.reset_index(drop=True)\n","        val_df = val_df.reset_index(drop=True)\n","        test_df = pd.DataFrame([], columns=df.columns)\n","\n","    # If test_df empty, you can set it equal to val_df or leave it empty - here we keep it empty if not provided.\n","    logger.info(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n","\n","    # ---------------------- Build datasets / loaders ----------------------\n","    train_ds = FruitDataset(train_df, ROOT_IMAGE_DIR, transform=train_transform, path_col=path_col, label_col=label_col)\n","    val_ds   = FruitDataset(val_df,   ROOT_IMAGE_DIR, transform=val_transform,   path_col=path_col, label_col=label_col)\n","    test_ds  = FruitDataset(test_df,  ROOT_IMAGE_DIR, transform=val_transform,   path_col=path_col, label_col=label_col) if len(test_df)>0 else None\n","\n","    # guard\n","    if len(train_ds) == 0:\n","        raise ValueError(\"Training dataset is empty. Check LABELS_CSV and 'split' values.\")\n","\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=(DEVICE.type=='cuda'))\n","    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type=='cuda'))\n","    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(DEVICE.type=='cuda')) if test_ds is not None else None\n","\n","    # ---------------------- Model setup ----------------------\n","    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","    num_ftrs = model.fc.in_features\n","    # num classes\n","    NUM_CLASSES = int(df[label_col].nunique())\n","    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n","    model = model.to(DEVICE)\n","\n","    # ---------------------- Optionally freeze backbone ----------------------\n","    def set_parameter_requires_grad(model, feature_extracting=True):\n","        if feature_extracting:\n","            for param in model.parameters():\n","                param.requires_grad = False\n","            for param in model.fc.parameters():\n","                param.requires_grad = True\n","\n","    FEATURE_EXTRACT = True\n","    set_parameter_requires_grad(model, feature_extracting=FEATURE_EXTRACT)\n","\n","    # ---------------------- Loss, Optimizer, Scheduler ----------------------\n","    criterion = nn.CrossEntropyLoss()\n","    params_to_update = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = optim.AdamW(params_to_update, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n","# ---------------------- Resume / checkpoint setup ----------------------\n","    start_epoch = 0\n","    best_val_acc = 0.0\n","    ckpt = find_latest_checkpoint(OUTPUT_DIR)\n","    if ckpt:\n","        try:\n","            print(\"loaded existing ckpt\")\n","            start_epoch, best_val_acc = load_checkpoint(ckpt, model, optimizer=optimizer, scheduler=scheduler, device=DEVICE)\n","            print(\"start_epoch:{} best_val_acc:{}\".format(start_epoch, best_val_acc))\n","            #logger.info(f\"Resuming from checkpoint. start_epoch={start_epoch}, best_val_acc={best_val_acc:.4f}\")\n","            print(f\"Resuming from checkpoint. start_epoch={start_epoch}, best_val_acc={best_val_acc:.4f}\")\n","        except Exception as e:\n","            logger.warning(f\"Failed to load checkpoint {ckpt}: {e}\\nStarting from scratch.\")\n","\n","    # ---------------------- Training loop ----------------------\n","    try:\n","        for epoch in range(start_epoch, NUM_EPOCHS):\n","            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n","\n","            # Optional quick eval on training set for sanity-check (small overhead)\n","            train_eval_loss, train_eval_acc = eval_one_epoch(model, train_loader, criterion, DEVICE)\n","\n","            val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, DEVICE)\n","\n","            scheduler.step()\n","\n","            logger.info(f\"Epoch {epoch+1}/{NUM_EPOCHS}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  \"\n","                        f\"eval_train_loss={train_eval_loss:.4f} eval_train_acc={train_eval_acc:.4f}  \"\n","                        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n","\n","            ckpt_state = {\n","                \"epoch\": epoch + 1,\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n","                \"best_val_acc\": best_val_acc\n","            }\n","            # save latest checkpoint always\n","            save_checkpoint(ckpt_state, os.path.join(OUTPUT_DIR, \"latest.pth\"))\n","\n","            # save best model\n","            if val_acc > best_val_acc:\n","                best_val_acc = val_acc\n","                best_path = os.path.join(OUTPUT_DIR, f\"best_epoch_{epoch+1:03d}_valacc_{val_acc:.4f}.pth\")\n","                save_checkpoint(ckpt_state, best_path)\n","                logger.info(f\"Saved new best model to {best_path} (val_acc={val_acc:.4f})\")\n","\n","    except Exception as e:\n","        # Save latest on exception to avoid losing progress\n","        tb = traceback.format_exc()\n","        logger.error(f\"Training failed unexpectedly: {e}\\n{tb}\")\n","        try:\n","            save_checkpoint({\n","                \"epoch\": epoch + 1 if 'epoch' in locals() else 0,\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n","                \"best_val_acc\": best_val_acc\n","            }, os.path.join(OUTPUT_DIR, \"latest_on_error.pth\"))\n","            logger.info(\"Saved latest checkpoint to latest_on_error.pth\")\n","        except Exception as se:\n","            logger.error(f\"Failed to save checkpoint on error: {se}\")\n","        raise\n","\n","    # ---------------------- Final evaluation on test set (optional) ----------------------\n","    # If a best model was saved, load it for test evaluation\n","    best_ckpt = None\n","    best_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"best_epoch_*.pth\")), key=os.path.getmtime, reverse=True)\n","    if best_files:\n","        best_ckpt = best_files[0]\n","    elif os.path.exists(os.path.join(OUTPUT_DIR, \"latest.pth\")):\n","        best_ckpt = os.path.join(OUTPUT_DIR, \"latest.pth\")\n","\n","    if best_ckpt:\n","        logger.info(f\"Loading best checkpoint for final evaluation: {best_ckpt}\")\n","        load_checkpoint(best_ckpt, model, device=DEVICE)\n","\n","    if test_loader is not None:\n","        test_loss, test_acc = eval_one_epoch(model, test_loader, criterion, DEVICE)\n","        logger.info(f\"TEST final: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n","    else:\n","        logger.info(\"No test split provided - skipping final test evaluation.\")\n","\n","    # ---------------------- Save class map for inference convenience ----------------------\n","    if os.path.exists(CLASSES_CSV):\n","        logger.info(f\"classes CSV exists at {CLASSES_CSV}\")\n","    else:\n","        # write small classes CSV mapping if not present\n","        uniq = sorted(df[label_col].unique())\n","        classes_df = pd.DataFrame({\"class_index\": list(range(len(uniq))), \"class_name\": [str(x) for x in uniq]})\n","        classes_df.to_csv(CLASSES_CSV, index=False)\n","        logger.info(f\"Wrote fallback classes CSV to {CLASSES_CSV}\")\n","\n","    logger.info(\"Training script finished.\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Q5zqte5GVJ_9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760802013003,"user_tz":240,"elapsed":31152,"user":{"displayName":"Carlos","userId":"17576890038933523144"}},"outputId":"22d9435b-c233-403c-efa8-523933de5e49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Refresh index: 100% (21/21), done.\n","On branch train-model\n","Your branch is ahead of 'origin/train-model' by 2 commits.\n","  (use \"git push\" to publish your local commits)\n","\n","It took 2.29 seconds to compute the branch ahead/behind values.\n","You can use '--no-ahead-behind' to avoid this.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   resnet_fruits_training.ipynb\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mclasses.gsheet\u001b[m\n","\t\u001b[31mfruits360_data.gsheet\u001b[m\n","\t\u001b[31mlabels.gsheet\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["'''!git config --global user.email \"he.carlitos@gmail.com\"\n","!git config --global user.name \"Carlos\"\n","!git add .\n","!git commit -m \"Ready to train\"\n","!git push https://cahehe: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@github.com/cahehe/fruit-image-classifier.git'''\n","# show current working directory and list files here\n","# show mount info + drive root listing\n","#%cd Projects/\n","#%cd Fruits\\ Image\\ Classifier\n","#!git commit -m \"Ready to train\"\n","#\n","!git add resnet_fruits_training.ipynb\n","!git commit -m \"saving latest changes to training\""]},{"cell_type":"code","source":[],"metadata":{"id":"iqxG7mbznjz3"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"jupytext":{"formats":"ipynb,py:percent","main_language":"python"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}